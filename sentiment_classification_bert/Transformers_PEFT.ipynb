{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a49445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip freeze > peft_requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78a43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import get_peft_model, PrefixTuningConfig, LoraConfig, TaskType\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 固定seed\n",
    "torch.manual_seed(42)\n",
    "# 确定设备：如果有GPU可用则使用GPU，否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#如果GPU可以，可以改为20\n",
    "num_epochs = 5\n",
    "patience = 5\n",
    "\n",
    "training_record = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723271c2",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca4c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer，加载bert的分词器,uncased就是不区分大小写\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# dataset，加载数据里\n",
    "dataset_sst2 = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\": \"./sst2/data/train-00000-of-00001.parquet\",\n",
    "        \"validation\": \"./sst2/data/validation-00000-of-00001.parquet\"\n",
    "        })\n",
    "\n",
    "# preprocessing\n",
    "def collate_fn(batch):\n",
    "    #对字符串文本，进行编码，变为id,longest就是最长，padding就是填充,truncation为True就是截断\n",
    "    inputs = tokenizer([x[\"sentence\"] for x in batch], padding=\"longest\", truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    return inputs, labels\n",
    "\n",
    "train_loader = DataLoader(dataset_sst2[\"train\"], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset_sst2[\"validation\"], batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd614995",
   "metadata": {},
   "source": [
    "# define evaluattion and training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba87e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():# 在评估过程中关闭梯度计算\n",
    "        total_samples = 0 #统计验证集总样本数量\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()} #输入是一个字典，所以拿value\n",
    "            labels = labels.to(device)\n",
    "            probs = model(**inputs)\n",
    "            probs = probs.logits.squeeze()\n",
    "            loss = F.cross_entropy(probs, labels.float()) #求损失\n",
    "            val_loss += loss.item()\n",
    "            val_acc += ((probs > 0.5) == labels).sum().item() #模型的预测结果与实际标签是否相等,求和得到预测正确数量\n",
    "            total_samples += len(labels)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= total_samples\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, device, num_epochs=3, patience=3):\n",
    "    # 将模型移动到指定设备\n",
    "    model.to(device)\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # 计算训练步数总数\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "    # 使用transformers库中的余弦学习率调度器进行学习率调整\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps), #前20%步，学习率提升\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 提前停止训练的控制变量\n",
    "    best_val_acc = -1\n",
    "    cur = 0\n",
    "\n",
    "    # 存储训练和验证指标的容器\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # 进入训练模式\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # 对训练数据进行迭代\n",
    "        for inputs, labels in train_loader:\n",
    "            # 将数据移动到指定设备\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 前向传播并计算损失\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(**inputs) # **代表字典解包，inputs 中的键名必须与模型 forward() 方法的参数名完全一致\n",
    "            probs = probs.logits.squeeze()\n",
    "            loss = F.binary_cross_entropy_with_logits(probs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # 收集指标\n",
    "            train_loss += loss.item()\n",
    "            train_acc += ((probs > 0.5) == labels).sum().item()\n",
    "            total_samples += len(labels)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc  /= total_samples\n",
    "\n",
    "        # 进行验证\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        # 记录指标\n",
    "        print(f\"epoch {epoch}: train_loss {train_loss:.4f}, train_acc {train_acc:.4f}, val_loss {val_loss:.4f}, val_acc {val_acc:.4f}\")\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # 提前停止训练\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            cur = 0\n",
    "        else:\n",
    "            cur += 1\n",
    "        if cur >= patience:\n",
    "            print(\"提前停止训练\")\n",
    "            break\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ae5e9",
   "metadata": {},
   "source": [
    "# Prefix Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928006d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,865,473 || all params: 119,348,482 || trainable%: 8.266106811480014\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,    # 序列分类\n",
    "    num_virtual_tokens=20,\n",
    "    prefix_projection=True,        # 默认 False；True 时可加 MLP 映射\n",
    "    encoder_hidden_size=512  # 设置 MLP 映射的维度\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # 查看可训练参数（<1%）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98ff476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cf4b318b2e42fd84f1c9b9aad6bf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss 0.6176, train_acc 0.5447, val_loss 52.8172, val_acc 0.8463\n",
      "epoch 1: train_loss 0.3238, train_acc 0.8643, val_loss 54.1661, val_acc 0.8761\n",
      "epoch 2: train_loss 0.2720, train_acc 0.8825, val_loss 59.6702, val_acc 0.8876\n",
      "epoch 3: train_loss 0.2515, train_acc 0.8924, val_loss 55.8352, val_acc 0.8945\n",
      "epoch 4: train_loss 0.2376, train_acc 0.8985, val_loss 57.7168, val_acc 0.9071\n",
      "epoch 5: train_loss 0.2264, train_acc 0.9038, val_loss 60.9866, val_acc 0.9083\n",
      "epoch 6: train_loss 0.2192, train_acc 0.9069, val_loss 60.5933, val_acc 0.9128\n",
      "epoch 7: train_loss 0.2144, train_acc 0.9093, val_loss 59.8683, val_acc 0.9128\n",
      "epoch 8: train_loss 0.2124, train_acc 0.9104, val_loss 61.1281, val_acc 0.9151\n",
      "epoch 9: train_loss 0.2114, train_acc 0.9111, val_loss 60.4365, val_acc 0.9151\n"
     ]
    }
   ],
   "source": [
    "training_record[\"prefix_tuning\"] = train(model, train_loader, val_loader, device, num_epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117ca53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2975dd",
   "metadata": {},
   "source": [
    "# P-tuning-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96196f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 590,593 || all params: 110,073,602 || trainable%: 0.5365437209913417\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,    # 序列分类\n",
    "    num_virtual_tokens=32,\n",
    "    prefix_projection=False        # 默认 False；True 时可加 MLP 映射\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # 查看可训练参数（<1%）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b96f457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889ee0c337264eeda06cb1045ef45831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss 0.6885, train_acc 0.4495, val_loss 54.7002, val_acc 0.4908\n",
      "epoch 1: train_loss 0.6847, train_acc 0.4463, val_loss 54.4352, val_acc 0.5046\n",
      "epoch 2: train_loss 0.6767, train_acc 0.4746, val_loss 54.0218, val_acc 0.6227\n",
      "epoch 3: train_loss 0.6650, train_acc 0.5310, val_loss 53.6759, val_acc 0.6686\n",
      "epoch 4: train_loss 0.6516, train_acc 0.5730, val_loss 53.4537, val_acc 0.6961\n",
      "epoch 5: train_loss 0.6401, train_acc 0.6063, val_loss 53.3117, val_acc 0.7190\n",
      "epoch 6: train_loss 0.6282, train_acc 0.6247, val_loss 53.2884, val_acc 0.7271\n",
      "epoch 7: train_loss 0.6220, train_acc 0.6339, val_loss 53.2778, val_acc 0.7317\n",
      "epoch 8: train_loss 0.6191, train_acc 0.6385, val_loss 53.2710, val_acc 0.7328\n",
      "epoch 9: train_loss 0.6170, train_acc 0.6435, val_loss 53.2746, val_acc 0.7317\n"
     ]
    }
   ],
   "source": [
    "training_record[\"P-tuning-v2\"] = train(model, train_loader, val_loader, device, num_epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d336f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f704db",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ab0730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 295,681 || all params: 109,778,690 || trainable%: 0.26934280232347463\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,    # 序列分类\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # 查看可训练参数（<1%）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c926ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88adf7b23f974922a2106b10cc3ae989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss 0.6996, train_acc 0.4427, val_loss 54.5700, val_acc 0.4908\n",
      "epoch 1: train_loss 0.4900, train_acc 0.7105, val_loss 54.1376, val_acc 0.8658\n",
      "epoch 2: train_loss 0.3167, train_acc 0.8694, val_loss 56.5489, val_acc 0.8784\n",
      "epoch 3: train_loss 0.2926, train_acc 0.8786, val_loss 58.1786, val_acc 0.8945\n",
      "epoch 4: train_loss 0.2769, train_acc 0.8844, val_loss 58.2912, val_acc 0.9014\n",
      "epoch 5: train_loss 0.2688, train_acc 0.8880, val_loss 58.3683, val_acc 0.9025\n",
      "epoch 6: train_loss 0.2638, train_acc 0.8896, val_loss 59.0802, val_acc 0.9025\n",
      "epoch 7: train_loss 0.2603, train_acc 0.8906, val_loss 59.2532, val_acc 0.9014\n",
      "epoch 8: train_loss 0.2589, train_acc 0.8905, val_loss 59.2620, val_acc 0.9060\n",
      "epoch 9: train_loss 0.2583, train_acc 0.8917, val_loss 59.1033, val_acc 0.9037\n"
     ]
    }
   ],
   "source": [
    "training_record[\"LoRA\"] = train(model, train_loader, val_loader, device, num_epochs=10, patience=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
